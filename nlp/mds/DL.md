
# 经典网络模型
Unet
Dit

# 损失函数

# 优化器
Momentum:
    优点：相较于普通的SGD，Momentum在更新过程中考虑了之前梯度的累积，从而可以加速收敛，减少震荡。
    缺点：对超参数（如动量系数）比较敏感。
    适用场景：适用于复杂的优化问题，特别是在非凸函数的训练中。
Adagrad
    优点：自动调整学习率，稀疏数据的特征更为显著。
    缺点：随着训练进程的进行，学习率会迅速降低，可能导致过早收敛。
    适用场景：适用于稀疏数据或者特征尺度差异较大的任务，如NLP或推荐系统中的问题。
RMSprop:
    优点：结合了Adagrad的优点，通过引入滑动平均来平滑学习率，避免了学习率过早下降的问题。
    缺点：同样需要调节超参数。
    适用场景：在处理非平稳目标（如RNN训练）时，表现优越。
Adam
    优点：结合了Momentum和RMSprop的优点，能够自适应调整每个参数的学习率，通常表现出色。
    缺点：在某些问题中，可能导致训练不稳定，尤其是当超参数选择不当时。
    适用场景：广泛应用于各类深度学习任务，尤其是在处理复杂模型和大规模数据集时非常有效。
